from dotenv import load_dotenv
import os
import streamlit as st
from langchain_openai.chat_models import ChatOpenAI
import time
import torch
import numpy as np
import shap
from streamlit_shap import st_shap
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Advice Generator",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)


@st.cache_resource
def load_model():
    model_path = "sentiment-bert-model2" 
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    return tokenizer, model, device

tokenizer, model, device = load_model()

model.config.id2label = {0: "negative", 1: "neutral", 2: "positive"}
model.config.label2id = {"negative": 0, "neutral": 1, "positive": 2}
sentiment_pipeline = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    framework="pt",
    device=0 if torch.cuda.is_available() else -1,
    return_all_scores=True
)
explainer = shap.Explainer(sentiment_pipeline)

# OpenAI GPT ëª¨ë¸ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
chat_model = ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY)


# ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    st.title("MENU")
    st.markdown("---")
    st.markdown("### âš™ï¸ ì„¤ì •")
    st.markdown("")
    model_type = st.selectbox(
        "ì‚¬ìš© ëª¨ë¸",
        ("gpt-3.5-turbo", "gpt-4")
    )
    st.markdown("")
    business_type = st.selectbox(
        "ë„ë©”ì¸ ì„ íƒ",
        ("í•­ê³µ", "ìˆ™ë°•", "ì‹ë‹¹", "ê¸ˆìœµ", "ì˜ë£Œ", "êµìœ¡")
    )
    st.markdown("---")
    st.markdown("### ğŸª„ ë„ì›€ë§")
    st.info("ê³ ê°ì˜ ì˜ê²¬ì„ ì…ë ¥ë€ì— ì…ë ¥í•˜ì‹œë©´ AIê°€ ë§ì¶¤í˜• ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")

# ë©”ì¸ í˜ì´ì§€
st.markdown("<h1 style='text-align: center; color: #4B89DC;'>Advice Generator</h1>", unsafe_allow_html=True)
#st.markdown("<h4 style='text-align: center; color: #666666;'>ê³ ê°ì˜ ì˜ê²¬ì„ ì…ë ¥í•˜ì‹œë©´, AIê°€ ë§ì¶¤í˜• ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤</h4>", unsafe_allow_html=True)


# íƒ­ ìƒì„±
tab1, tab2 = st.tabs(["ğŸ“ ì¡°ì–¸ ìš”ì²­", "â„¹ï¸ ì‚¬ìš© ë°©ë²•"])

with tab1:
    st.markdown("### ì˜ê²¬ ì…ë ¥")
    with st.container():        
        contents = st.text_area("", height=150, placeholder="ê³ ê°ì˜ ì˜ê²¬ì„ ì…ë ¥í•´ì£¼ì„¸ìš” ...")
        
        def predict(contents):
            inputs = tokenizer(contents, return_tensors="pt", truncation=True, max_length=128).to(device)
            with torch.no_grad():
                outputs = model(**inputs)
                probabilities = outputs.logits.softmax(dim=-1)
            return probabilities.cpu().numpy()

        def model_forward(contents):
            if isinstance(contents, list):
                contents = [text if isinstance(text, str) else "" for text in contents]
            else:
                contents = [contents]

            encoded_inputs = tokenizer(contents, padding=True, truncation=True, max_length=128, return_tensors="pt")
            encoded_inputs = {key: tensor.to(device) for key, tensor in encoded_inputs.items()}
            with torch.no_grad():
                output = model(**encoded_inputs)
                return output.logits.cpu().numpy()
        
        col1, col2, col3 = st.columns([1, 1, 1])
        with col2:
            analyze_button = st.button("ğŸ” ì¡°ì–¸ ë°›ê¸°", use_container_width=True)
        
        st.markdown("</div>", unsafe_allow_html=True)
    
    # ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ AI ì¡°ì–¸ ìƒì„±
    if analyze_button:
        if contents.strip():
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°”ë¡œ ë¡œë”© í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i in range(100):
                # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                progress_bar.progress(i + 1)
                if i < 30:
                    status_text.text("ğŸ§ ê³ ê° ì˜ê²¬ ë¶„ì„ ì¤‘ ...")
                elif i < 70:
                    status_text.text("ğŸ«¨ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ...")
                elif i < 99:
                    status_text.text("ğŸ¤© ìµœì¢… ê²°ê³¼ ì¤€ë¹„ ì¤‘ ...")
                else:
                    status_text.text("")
                time.sleep(0.03)

            # inputs = tokenizer(contents, return_tensors="pt", truncation=True, max_length=128)
            # input_ids = inputs["input_ids"].to(device)
            # attention_mask = inputs["attention_mask"].to(device)
            # with torch.no_grad():
            #     outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            #     probabilities = outputs.logits.softmax(dim=-1).cpu().numpy()
            # pred_label = np.argmax(probabilities)
            # label_map = {0: "negative", 1: "neutral", 2: "positive"}

            ### ìˆ˜ì • ë¶€ë¶„
            result = sentiment_pipeline(contents)
            pred_label = max(result[0], key=lambda x: x["score"])["label"]
            # shap ì‹œê°í™”
            shap_values = explainer([contents])
            #shap.plots.text(shap_values[0])  # ì²« ë²ˆì§¸ ë¬¸ì¥
            def st_shap(plot_html, height=None, width=None):
            # plot_htmlì€ ì´ì œ HTML ë¬¸ìì—´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
                full_html = f"<head>{shap.getjs()}</head><body>{plot_html}</body>"
                components.html(full_html, height=height, width=width)
            shap_html = shap.plots.text(shap_values)
            st_shap(shap_html, height=400)
            
            # í”„ë¡¬í”„íŠ¸
            prompt = f"""
            "{contents}"

            ìœ„ ì˜ê²¬ì€ ê°ì • ë¶„ì„ì„ í†µí•´ "{pred_label}"  ê°ì •ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.

            ë‹¹ì‹ ì˜ ì—­í• :  
            - ì…ë ¥ëœ {business_type}ë¥¼ ê³ ë ¤í•˜ì—¬ ë‹µë³€ì„ ì‘ì„±í•©ë‹ˆë‹¤.
            - ì„œë¹„ìŠ¤ ê°œì„ ì„ ìœ„í•œ ì „ëµì  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.  
            - ê°ì •ì„ ê³ ë ¤í•˜ë˜, ê°ì •ì ì¸ ë°˜ì‘ë³´ë‹¤ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ì  í•´ê²°ì±…ì„ ì œì•ˆí•©ë‹ˆë‹¤.  
            - ê³ ê°ì˜ ë¶ˆë§Œ ì‚¬í•­ì´ íšŒì‚¬ì˜ ìš´ì˜ì— ë¯¸ì¹  ì˜í–¥ì„ ë¶„ì„í•˜ê³ , ì‹¤ìš©ì ì¸ ëŒ€ì‘ ì „ëµì„ ì œê³µí•©ë‹ˆë‹¤.  
            - í•„ìš”í•˜ë©´ ì—…ê³„ ì‚¬ë¡€, ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

            ì‘ë‹µ ì˜ˆì‹œ: 
            - ê°ì •ì´ "negative"ì´ë©´: ë¬¸ì œì˜ ì›ì¸ì„ ë¶„ì„í•˜ê³  íšŒì‚¬ê°€ ê°œì„ í•  ìˆ˜ ìˆëŠ” ì „ëµì  ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.  
            - ê°ì •ì´ "positive"ì´ë©´: ê³ ê° ê²½í—˜ì„ ë”ìš± ê°•í™”í•  ë°©ë²•ì„ ì œì•ˆí•˜ì„¸ìš”.  
            - ê°ì •ì´ "neutral"ì´ë©´: ì¶”ê°€ì ì¸ ê³ ê° í”¼ë“œë°±ì„ ìœ ë„í•˜ê³ , ì„œë¹„ìŠ¤ ê°œì„ ì˜ ê¸°íšŒë¥¼ ì°¾ì•„ ì œì•ˆí•˜ì„¸ìš”.
            - ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
              1. ë¬¸ì œì˜ ì›ì¸ ë¶„ì„ :
              2. ì‹¤ì§ˆì ì¸ í•´ê²°ì±… ì œì•ˆ :
              3. ê´€ë ¨ëœ ì—…ê³„ ì‚¬ë¡€ë‚˜ ì°¸ê³ í•  ë§Œí•œ ì „ëµ :

            ì´ì œ ë¶„ì„ì ì¸ ì¸ì‚¬ì´íŠ¸ì™€ ì ìš© ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì´ë©° ì‹¤ìš©ì ì¸ ì „ëµì„ í¬í•¨í•˜ì—¬ ì¡°ì–¸ì„ ì‘ì„±í•˜ì„¸ìš”:
            """
            
            response = chat_model.predict(prompt)
            
            # ê²°ê³¼ ì¶œë ¥
            with st.container():
                st.markdown("### ğŸ’¡ AI Advice")
                
                # ê²°ê³¼ ì»¨í…Œì´ë„ˆ ì‹œì‘
                st.markdown('<div class="result-container">', unsafe_allow_html=True)
                
                # ê²°ê³¼ ë‚´ìš© í‘œì‹œ
                st.markdown(response)
                
                # ê²°ê³¼ ì»¨í…Œì´ë„ˆ ì¢…ë£Œ
                st.markdown('</div>', unsafe_allow_html=True)
                
                # ê²°ê³¼ í”¼ë“œë°±
                st.markdown("")
                st.markdown("##### ë‹µë³€ì´ ë§ˆìŒì— ë“œì…¨ë‚˜ìš”?")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.button("ğŸ‘ ìœ ìš©í•´ìš”", use_container_width=True)
                with col2:
                    st.button("ğŸ‘ ì•„ì‰¬ì›Œìš”", use_container_width=True)
                with col3:
                    st.button("ğŸ“‹ ì €ì¥í•˜ê¸°", use_container_width=True)
        else:
            st.error("â— ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")

        

with tab2:
    st.markdown("### ì‚¬ìš© ë°©ë²•")
    st.markdown("""
    1. **ê³ ê° ì˜ê²¬ ì…ë ¥**: ë¶„ì„í•˜ê³  ì‹¶ì€ ê³ ê°ì˜ ì˜ê²¬ì´ë‚˜ í”¼ë“œë°±ì„ ì…ë ¥ì°½ì— ì‘ì„±í•©ë‹ˆë‹¤.
    2. **ì¡°ì–¸ ë°›ê¸° í´ë¦­**: ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ AIê°€ ì˜ê²¬ì„ ë¶„ì„í•˜ê³  ë§ì¶¤í˜• ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.
    3. **ê²°ê³¼ í™•ì¸**: ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒ ì„¸ ê°€ì§€ í•­ëª©ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:
       - ë¬¸ì œ ì›ì¸ ë¶„ì„
       - ì‹¤ì§ˆì ì¸ í•´ê²°ì±… ì œì•ˆ
       - ê´€ë ¨ëœ ì—…ê³„ ì‚¬ë¡€ë‚˜ ì°¸ê³ í•  ë§Œí•œ ì „ëµ
    4. **í”¼ë“œë°±**: ê²°ê³¼ì— ëŒ€í•œ í”¼ë“œë°±ì„ ì œê³µí•˜ì—¬ ì„œë¹„ìŠ¤ ê°œì„ ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    st.info("ğŸ’¡ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì…ë ¥ë€ì— ìƒì„¸í•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.")

# í‘¸í„°
st.markdown("---")
st.markdown("<p style='text-align: center; color: #888888;'>Â© 2025 AI Advice Generator</p>", unsafe_allow_html=True)

# ì»¤ìŠ¤í…€ CSS ì¶”ê°€
st.markdown("""
<style>
    .stButton>button {
        background-color: #4B89DC;
        color: white;
        border-radius: 5px;
        border: none;
        padding: 10px 15px;
    }
    .stButton>button:hover {
        background-color: #3A70B9;
    }
    .stTextArea>div>div>textarea {
        border-radius: 5px;
        border: 1px solid #ddd;
    }
    .stProgress>div>div>div {
        background-color: #4B89DC;
    }
    *:hover {
        color: inherit !important;
    }
</style>
""", unsafe_allow_html=True)
